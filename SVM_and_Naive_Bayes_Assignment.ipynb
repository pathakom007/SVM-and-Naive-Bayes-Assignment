{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "1. What is Information Gain, and how is it used in Decision Trees?\n",
        "\n",
        "   - Information Gain (IG) is a metric used in Decision Trees to decide which feature to split on at each step.\n",
        "   - It measures how much uncertainty (impurity) is reduced after splitting a dataset using a particular feature.\n",
        "\n",
        "   How Information Gain Is Used in Decision Trees:\n",
        "   - Calculate entropy of the entire dataset\n",
        "   - For each feature:\n",
        "     - Split the dataset\n",
        "     - Calculate entropy of each split\n",
        "     - Compute Information Gain\n",
        "   - Choose the feature with maximum Information Gain\n",
        "   - Repeat recursively until:\n",
        "     - Data is pure, or\n",
        "     - No features remain\n",
        "\n"
      ],
      "metadata": {
        "id": "hAsCQOQdxqlu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "2. What is the difference between Gini Impurity and Entropy?\n",
        "\n",
        "   - Entropy:\n",
        "     - Measure of uncertainity\n",
        "     - Uses logarithms\n",
        "     - Computation speed is slower\n",
        "     - Max Value(Binary) is 1\n",
        "     - More sensitive to class changes\n",
        "\n",
        "   - Gini Impurity:\n",
        "     - Probability of misclassification\n",
        "     - Uses squares\n",
        "     - Computation speed is Faster\n",
        "     - Max Value(Binary) is 0.5\n",
        "     - Less sensitive"
      ],
      "metadata": {
        "id": "tbn6iNvyyTV9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "3. What is Pre-Pruning in Decision Trees?\n",
        "\n",
        "   - Pre-Pruning (also called Early Stopping) is a technique used to stop the growth of a Decision Tree early—before it becomes too deep or complex."
      ],
      "metadata": {
        "id": "p2WugNcCzKN-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "4. Write a Python program to train a Decision Tree Classifier using Gini\n",
        "Impurity as the criterion and print the feature importances (practical)."
      ],
      "metadata": {
        "id": "X-2suyEzzW_1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import required libraries\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "import pandas as pd\n",
        "\n",
        "# Load a sample dataset (Iris)\n",
        "data = load_iris()\n",
        "X = data.data          # Features\n",
        "y = data.target        # Target labels\n",
        "feature_names = data.feature_names\n",
        "\n",
        "# Create Decision Tree model using Gini Impurity\n",
        "model = DecisionTreeClassifier(\n",
        "    criterion='gini',\n",
        "    random_state=42\n",
        ")\n",
        "\n",
        "# Train the model\n",
        "model.fit(X, y)\n",
        "\n",
        "# Get feature importances\n",
        "importances = model.feature_importances_\n",
        "\n",
        "# Display feature importances clearly\n",
        "feature_importance_df = pd.DataFrame({\n",
        "    'Feature': feature_names,\n",
        "    'Importance': importances\n",
        "}).sort_values(by='Importance', ascending=False)\n",
        "\n",
        "print(feature_importance_df)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YYvHKNerzb2A",
        "outputId": "bf263615-556c-4c28-f427-6b0a0eb6d8d9"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "             Feature  Importance\n",
            "2  petal length (cm)    0.564056\n",
            "3   petal width (cm)    0.422611\n",
            "0  sepal length (cm)    0.013333\n",
            "1   sepal width (cm)    0.000000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "5. What is a Support Vector Machine (SVM)?\n",
        "\n",
        "   - A Support Vector Machine (SVM) is a supervised machine learning algorithm used for classification and regression, mainly known for finding the best possible decision boundary between data points of different classes."
      ],
      "metadata": {
        "id": "vZ4codsfzypB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "6. What is the Kernel Trick in SVM?\n",
        "\n",
        "   - The Kernel Trick is a powerful technique used in Support Vector Machines (SVMs) that allows them to solve non-linearly separable problems by implicitly mapping data into a higher-dimensional space—without actually computing that transformation."
      ],
      "metadata": {
        "id": "VDCj6gV10D0L"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "7. Write a Python program to train two SVM classifiers with Linear and RBF\n",
        "kernels on the Wine dataset, then compare their accuracies.\n"
      ],
      "metadata": {
        "id": "LOXmPZLB0N9d"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import required libraries\n",
        "from sklearn.datasets import load_wine\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load Wine dataset\n",
        "data = load_wine()\n",
        "X = data.data\n",
        "y = data.target\n",
        "\n",
        "# Split data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.3, random_state=42\n",
        ")\n",
        "\n",
        "# Train SVM with Linear Kernel\n",
        "svm_linear = SVC(kernel='linear')\n",
        "svm_linear.fit(X_train, y_train)\n",
        "\n",
        "# Predict and calculate accuracy\n",
        "y_pred_linear = svm_linear.predict(X_test)\n",
        "linear_accuracy = accuracy_score(y_test, y_pred_linear)\n",
        "\n",
        "# Train SVM with RBF Kernel\n",
        "svm_rbf = SVC(kernel='rbf')\n",
        "svm_rbf.fit(X_train, y_train)\n",
        "\n",
        "# Predict and calculate accuracy\n",
        "y_pred_rbf = svm_rbf.predict(X_test)\n",
        "rbf_accuracy = accuracy_score(y_test, y_pred_rbf)\n",
        "\n",
        "# Print accuracies\n",
        "print(\"Linear Kernel SVM Accuracy:\", linear_accuracy)\n",
        "print(\"RBF Kernel SVM Accuracy:\", rbf_accuracy)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uNDexk3a0S16",
        "outputId": "f0c13eac-82d1-48f4-a2ce-c0707f8dce06"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Linear Kernel SVM Accuracy: 0.9814814814814815\n",
            "RBF Kernel SVM Accuracy: 0.7592592592592593\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "- Linear:\n",
        "  - Accuracy is higher/similar\n",
        "  - To use when data is linearly seperable\n",
        "- RBF:\n",
        "  - Accuracy is slightly lower or higher\n",
        "  - To use when data is Complex or non-linear"
      ],
      "metadata": {
        "id": "MaNOnjTO0qbS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "8. What is the Naïve Bayes classifier, and why is it called \"Naïve\"?\n",
        "\n",
        "   - Naïve Bayes is a supervised probabilistic classification algorithm based on Bayes’ Theorem.\n",
        "   - It predicts the class of a data point by calculating the posterior probability of each class given the input features and choosing the class with the highest probability.\n",
        "\n",
        "   Why Is It Called “Naïve”?\n",
        "   - The Naive Bayes classifier is called \"naive\" because it makes a strong, often unrealistic, assumption that all features in a dataset are independent of each other."
      ],
      "metadata": {
        "id": "wn7HIunf1R__"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "9. Explain the differences between Gaussian Naïve Bayes, Multinomial Naïve\n",
        "Bayes, and Bernoulli Naïve Bayes.\n",
        "\n",
        "  -  Gaussian Naïve Bayes (GNB):\n",
        "     - Data Type is Continuos.\n",
        "     - Normal(Gausian) Distribution.\n",
        "     - Input values is any real number.\n",
        "\n",
        "  -  Multinomial Naïve Bayes (MNB):\n",
        "     - Count-baseddata type.\n",
        "     - Multinomial Distribution.\n",
        "     - Input values are 0,1,2,...\n",
        "\n",
        "  -  Bernoulli Naïve Bayes (BNB):\n",
        "     - Binary data type.\n",
        "     - Bernoulli Distribution.\n",
        "     - Input values are 0 or 1."
      ],
      "metadata": {
        "id": "9b22oA1E1vtE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "10. Write a Python program to train a Gaussian Naïve Bayes classifier on the Breast Cancer\n",
        "dataset and evaluate accuracy"
      ],
      "metadata": {
        "id": "ddJ_cIlV3Gk6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import required libraries\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load Breast Cancer dataset\n",
        "data = load_breast_cancer()\n",
        "X = data.data\n",
        "y = data.target\n",
        "\n",
        "# Split data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.3, random_state=42\n",
        ")\n",
        "\n",
        "# Train Gaussian Naïve Bayes classifier\n",
        "gnb = GaussianNB()\n",
        "gnb.fit(X_train, y_train)\n",
        "\n",
        "# Predict on test data\n",
        "y_pred = gnb.predict(X_test)\n",
        "\n",
        "# Calculate accuracy\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "\n",
        "# Print accuracy\n",
        "print(\"Gaussian Naïve Bayes Accuracy:\", accuracy)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bkcVdno73fR2",
        "outputId": "8b566b33-89c3-4904-c0c7-f7e5d6171ce0"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Gaussian Naïve Bayes Accuracy: 0.9415204678362573\n"
          ]
        }
      ]
    }
  ]
}